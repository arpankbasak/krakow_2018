---
author: "Ben Bolker"
title: "statistical essentials for modeling"
---

## What do we mean by statistical inference?

- answering **scientific questions**
    - most important: clear, well-posed questions (theory)
    - next: good experimental design
	- last: statistical practice
	- ... but all are necessary

all of these questions should be connected!

- statistics for:
    - quantifying best guesses (point estimates)
	- quantifying uncertainty (confidence intervals)
	- statements about *clarity* (statistical significance testing)

## reproducibility crisis

many scientific results are unreproducible

- lack of openness (data/methods)
- questionable research practices (QRPs)
- p-hacking; snooping (Simonsohn); "Texas sharpshooter fallacy"
- "garden of forking paths" (Gelman)

**analytic decisions must be made independently of the data**

## power analysis

- experimental design
- in base R: `apropos("power")`
- [Russ Length home page](http://homepage.stat.uiowa.edu/~rlenth/Power/oldversion.html)
- most power analyses will be crude/order-of-magnitude
- think about *biological* effect sizes: what is the *smallest* effect that would be biologically interesting?
- need to specify effects and variances (standard deviations)

## goals of analysis (Harrell)

- exploration
- prediction
- inference

## exploration

- looking for patterns *only*
- no p-values at all?
- confidence intervals (perhaps),  
but taken with an inferential grain of salt

## prediction

- want quantitative answers about specific cases
- algorithmic approaches (Breiman)
- penalized approaches:  
automatically collapse
- confidence intervals challenging

## inference

*qualitative* statements about clarity and importance of effects:

- effects that are distinguishable from null hypothesis of noise
- test among discrete hypotheses (danger sign)

*quantitative* statements:

- relative strength/magnitude of effects
- importance (e.g. fraction variance explained)

## how much data do you need for a given model?

- link to video
- rule of thumb: 10-20 per data point
- rules for continuous, count, binomial data
- counting data points/"degrees of freedom" for clustered data?

## dimension reduction

- must be *a priori*
- discard interactions
- simplify questions
- collapse variables, e.g. by PCA

## modes of inference (Bolker chapter 6)

- Wald vs. likelihood ratio vs. Bayesian
- information-theoretic (AIC etc.) methods
- single vs. multiple parameters
- finite-size vs asymptotic
